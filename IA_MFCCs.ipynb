{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from keras.layers import LSTM, Dense, Dropout, Conv1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparativos antes de la RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(file_path,max_len):\n",
    "    # Cargar el audio\n",
    "    signal, sr = librosa.load(file_path,sr=96000)\n",
    "    # Realizar preénfasis\n",
    "    #filter_audio = librosa.effects.preemphasis(signal)\n",
    "    # Extraer MFCCs\n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=13)\n",
    "\n",
    "    # Realizar padding o recorte\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        num_zeros = max_len - mfcc.shape[1]\n",
    "        padded_mfcc = np.pad(mfcc, ((0, 0), (0, num_zeros)), mode='constant', constant_values=0)\n",
    "        return padded_mfcc\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "        return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'Data'\n",
    "LABELS = ['happy','cat', 'bed']\n",
    "\n",
    "mfccs = []\n",
    "labels = []\n",
    "\n",
    "padding = 500\n",
    "\n",
    "for label in LABELS:\n",
    "    path_file = DATA_DIR + f'/{label}'\n",
    "    for file in os.listdir(path_file):\n",
    "        file_path = path_file + f'/{file}'\n",
    "        \n",
    "        mfcc = preprocess_audio(file_path,padding)\n",
    "        \n",
    "        mfccs.append(mfcc)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos las listas a numpy array\n",
    "mfccs_array = np.array(mfccs) #Matriz 3d (5187, 13, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificación realizada:\n",
      "bed -> 0\n",
      "cat -> 1\n",
      "happy -> 2\n"
     ]
    }
   ],
   "source": [
    "# Convertir etiquetas a números\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "\n",
    "print(\"Codificación realizada:\")\n",
    "for idx, label in enumerate(le.classes_):\n",
    "    print(f\"{label} -> {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(mfccs_array, labels_encoded, test_size=0.2, random_state=312)\n",
    "\n",
    "# Crear modelo LSTM (Long short-term memory)\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(128),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')  # 3 clases: 'happy', 'cat', 'bed'\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "130/130 [==============================] - 10s 47ms/step - loss: 0.8790 - accuracy: 0.5932 - val_loss: 0.6872 - val_accuracy: 0.7245\n",
      "Epoch 2/15\n",
      "130/130 [==============================] - 4s 34ms/step - loss: 0.6342 - accuracy: 0.7428 - val_loss: 0.6213 - val_accuracy: 0.7418\n",
      "Epoch 3/15\n",
      "130/130 [==============================] - 4s 34ms/step - loss: 0.5430 - accuracy: 0.7795 - val_loss: 0.5153 - val_accuracy: 0.8054\n",
      "Epoch 4/15\n",
      "130/130 [==============================] - 4s 33ms/step - loss: 0.4908 - accuracy: 0.8067 - val_loss: 0.4838 - val_accuracy: 0.8054\n",
      "Epoch 5/15\n",
      "130/130 [==============================] - 4s 34ms/step - loss: 0.4457 - accuracy: 0.8248 - val_loss: 0.4221 - val_accuracy: 0.8449\n",
      "Epoch 6/15\n",
      "130/130 [==============================] - 4s 34ms/step - loss: 0.4427 - accuracy: 0.8303 - val_loss: 0.4561 - val_accuracy: 0.8198\n",
      "Epoch 7/15\n",
      "130/130 [==============================] - 5s 36ms/step - loss: 0.4165 - accuracy: 0.8315 - val_loss: 0.4109 - val_accuracy: 0.8507\n",
      "Epoch 8/15\n",
      "130/130 [==============================] - 5s 36ms/step - loss: 0.3802 - accuracy: 0.8520 - val_loss: 0.3718 - val_accuracy: 0.8487\n",
      "Epoch 9/15\n",
      "130/130 [==============================] - 5s 36ms/step - loss: 0.3556 - accuracy: 0.8619 - val_loss: 0.3591 - val_accuracy: 0.8690\n",
      "Epoch 10/15\n",
      "130/130 [==============================] - 5s 35ms/step - loss: 0.3624 - accuracy: 0.8592 - val_loss: 0.3831 - val_accuracy: 0.8555\n",
      "Epoch 11/15\n",
      "130/130 [==============================] - 4s 35ms/step - loss: 0.3529 - accuracy: 0.8621 - val_loss: 0.3687 - val_accuracy: 0.8622\n",
      "Epoch 12/15\n",
      "130/130 [==============================] - 5s 35ms/step - loss: 0.3415 - accuracy: 0.8718 - val_loss: 0.3517 - val_accuracy: 0.8719\n",
      "Epoch 13/15\n",
      "130/130 [==============================] - 5s 36ms/step - loss: 0.3153 - accuracy: 0.8817 - val_loss: 0.3859 - val_accuracy: 0.8632\n",
      "Epoch 14/15\n",
      "130/130 [==============================] - 5s 36ms/step - loss: 0.3349 - accuracy: 0.8725 - val_loss: 0.3655 - val_accuracy: 0.8642\n",
      "Epoch 15/15\n",
      "130/130 [==============================] - 5s 36ms/step - loss: 0.3018 - accuracy: 0.8846 - val_loss: 0.3597 - val_accuracy: 0.8584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d5bd57ec90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 12ms/step - loss: 0.3597 - accuracy: 0.8584\n",
      "Loss: 0.3596942126750946\n",
      "Accuracy: 0.8583815097808838\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_bed.wav\n",
      "1/1 [==============================] - 1s 797ms/step\n",
      "bed : 50.9693443775177%\n",
      "cat : 43.843233585357666%\n",
      "happy : 5.187417566776276%\n",
      "\n",
      "La palabra predicha es: bed\n",
      "\n",
      "audio_cat.wav\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "bed : 45.105159282684326%\n",
      "cat : 45.65918743610382%\n",
      "happy : 9.235657006502151%\n",
      "\n",
      "La palabra predicha es: cat\n",
      "\n",
      "audio_happy.wav\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "bed : 42.20638573169708%\n",
      "cat : 50.805866718292236%\n",
      "happy : 6.987746059894562%\n",
      "\n",
      "La palabra predicha es: cat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_paths = ['audio_bed.wav','audio_cat.wav','audio_happy.wav']\n",
    "\n",
    "for file_path in file_paths:\n",
    "    print(file_path)\n",
    "    preprocessed_audio = preprocess_audio(file_path,padding)\n",
    "    preprocessed_audio = preprocessed_audio.reshape(1, preprocessed_audio.shape[0], preprocessed_audio.shape[1])  # Convertir a formato (1, features, tiempo)\n",
    "    prediction = model.predict(preprocessed_audio)\n",
    "\n",
    "    print(f\"{le.inverse_transform([0])[0]} : {prediction[0][0] * 100}%\")\n",
    "    print(f\"{le.inverse_transform([1])[0]} : {prediction[0][1] * 100}%\")\n",
    "    print(f\"{le.inverse_transform([2])[0]} : {prediction[0][2] * 100}%\")\n",
    "\n",
    "    predicted_label_encoded = np.argmax(prediction, axis=1)[0]\n",
    "    predicted_label = le.inverse_transform([predicted_label_encoded])[0]\n",
    "    print(f\"\\nLa palabra predicha es: {predicted_label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "model.save('GUI/modelo.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
